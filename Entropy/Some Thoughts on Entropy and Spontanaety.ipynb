{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "genetic-slide",
   "metadata": {},
   "source": [
    "Some Thoughts on Entropy and Spontanaety\n",
    "========================================\n",
    "By: Matthew Rowley\n",
    "\n",
    "Outline\n",
    "-------\n",
    "* It's All About Entropy\n",
    "* It's Actually All About Heat\n",
    "* Statistical vs. Thermodynamic Definitions of Entropy\n",
    "* Schrodinger, Entropy, and Biological Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-escape",
   "metadata": {},
   "source": [
    "It's All About Entropy\n",
    "----------------------\n",
    "The function that determines spontanaety (at least at constant pressure) is Gibbs Energy: $\\Delta G=\\Delta H-T\\Delta S\\leq0$ for spontaneous processes\n",
    "\n",
    "A cursory look at that equation shows that both entropy and enthalpy play a role in the spontanaety of a process, but it is instructive to look back at the original formula for spontanaety, the second law of thermodynamics. For all processes ever observed to-date, $\\Delta S_{Universe}\\geq 0$\n",
    "\n",
    "$\\Delta S_{Universe}$ can be broken down into system and surroundings, giving us the following condition for spontanaety: $\\Delta S_{Surroundings} + \\Delta S_{System}\\geq0$\n",
    "\n",
    "Now the enthalpy change for the surroudings is just the negative of the system enthalpy over temperature (assuming a constant temperature), so the equation now becomes: $\\frac{-\\Delta H_{System}}{T} + \\Delta S_{System}\\geq0$\n",
    "\n",
    "Finally, multiply everything by $-T$ (remembering to switch $\\geq$ to $\\leq$ because of the negative!) to get us back to the original: $\\Delta H_{System} -T\\Delta S_{System} \\leq 0$\n",
    "\n",
    "Gibbs energy is probably better defined as $G=U+pV-TS$, but the above derivation shows why Gibbs energy is associated with spontanaety. It also reveals how the enthalpy term in the more familiar form of $\\Delta G=\\Delta H-T\\Delta S$ is really just accounting for the entropy of the surroundings. Entropy, and entropy alone determines spontanaety. Many familiar spontaneous chemical reactions are exothermic, but they are spontaneous because the released heat greatly increases the entropy of the surroundings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-northwest",
   "metadata": {},
   "source": [
    "It's Actually All About Heat\n",
    "----------------------------\n",
    "The second law of thermodynamics is a fundamental postulate, but is nothing more nor less than an observation written in the language of mathematics. It offers no commentary or explanation about *why* the entropy of the universe seems to ratchet only upward. When we take this postulate as true, we are able to use it (with the other laws of thermodynamics) to bootstrap the entire field of thermodynamics.\n",
    "\n",
    "We can perhaps gain some useful insight by carefully stepping away from the math and noting some less rigorous observations. Energy can take many different forms, such as:\n",
    "* gravitational potential energy\n",
    "* directed kinetic energy (coordinated molecular motions that result in macroscopic motion of the whole)\n",
    "* electromagnetic potential energy\n",
    "* electrochemical potential energy (a subset of the previous)\n",
    "* heat energy (random molecular motions which increase the temperature but don't result in any macroscopic motion)\n",
    "\n",
    "Most of these forms of energy can be transferred into the other forms with arbitrarily high efficiency. Any practical losses are due to system non-idealities such as friction, electrical resistance, etc. The exception to this rule is heat. To change heat into directed kinetic energy (as in a heat engine), or into gravitational potential energy (as in a hot air balloon), or into electromagnetic potential energy (as in a Peltier junction), there is a fundamental limit to the efficiency of the transformation. This limit exists even when friction and electrical resistance are taken to be zero! This makes heat energy occupy a special place compared to its peers. It has lost some potential to do useful work. Each transaction with heat as a currency must pay a tax to the cold sink of the system. This means that 10J of heat have less buying power (for useful work) compared to 10J of any other type of energy! It's worth noting, also, that the real inefficiencies in other systems due to friction and electrical resistance always manifest as those other kinds of energy transforming into useless, insipid heat.\n",
    "\n",
    "I think there is something profound about this distinction between heat and all other types of energy. Heat energy is just kinetic energy that has lost all direction and is distributed into all types of movement (vibrations, rotations, and translations), in all directions randomly. This randomness means all that motion doesn't accomplish much. . . the system doesn't *go* anywhere no matter how much heat energy it has, because the individual microscopic motions are, on average, all working against each other.\n",
    "\n",
    "Above, I showed how spontanaety was all about entropy, rather than really having anything to do with heat. Now I am suggesting that entropy, is really all about heat in the end anyway. Recall the original definition of entropy: $\\mathrm{d}S = \\frac{\\mathrm{d}q}{T}$. Whenever any form of energy converts to heat, the ratchet of entropy cranks upward because that heat cannot be converted back again (at least, not at 100% efficiency and not without a cold sink to absorb the losses). All the energy of the universe is slowly but irreversibly being converted into heat energy. Of course, this is not really my observation at all but it has been long known. In fact, an older version of the second law of thermodynamics states that heat energy will never be directly converted into kinetic energy with no losses. That *is* the law, not a consequence of the law, and it has been shown to be mathematically consistent with the $\\Delta S_{universe}\\geq0$ formulation of the same law."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-mother",
   "metadata": {},
   "source": [
    "Statistical vs. Thermodynamic Definitions of Entropy\n",
    "----------------------------------------------------\n",
    "\n",
    "The most common way to talk about entropy uses terms like \"chaos\" and \"disorder.\" This perspective is called the \"Statistical Definition of Entropy,\" and was expressed mathematically by Boltzmann as: $S=k_{B}\\ln\\Omega$\n",
    "\n",
    "Consider a mole of graphitic carbon at $298~K$. How would be calculate the entropy of this system using the statistical definition? How could we possibly account for all of the possible configurations of microstates of such a large system at room temperature (i.e. calculate the value of $\\Omega$)? We would have to enumerate all the system's states (vibrational, electronic, nuclear, etc.), and calculate their Boltzmann factor: $g_ie^{-\\Beta E_i}$. This might become even more difficult when you consider that for a solid like graphite the vibrational modes are not discrete, but form continuous bands (phonons). This problem is not not necessarily intractable. One simple way to find the density of states is to measure the heat capacity over all temperature ranges. The equipartition theorem states that heat energy is divided between all energetic modes of a system, so as more modes become active the heat capacity should increase. Experimentally, a graphite sample could be cooled to near absolute zero, then slowly heated while monitoring $q$ and $T$ to find the heat capacity.\n",
    "\n",
    "Now we must take a brief detour to note the other, original definition of entropy. Entropy was first recognized as a state function while physicists were working to characterize the properties of heat pumps and heat engines. It was defined originally as $\\mathrm{d}S=\\frac{\\mathrm{d}q}{T}$. This equation is often integrated to give the isothermic entropy change as: $\\Delta S=\\frac{q}{T}$. The third law of thermodynamics defines the entropy of a perfect crystal at $0K$ as $0\\frac{J}{K}$, and gives us the anchor to define an absolute entropy. Integrate the original definition as shown below, and you can get the entropy at $298K$\n",
    "\n",
    "$\\int\\mathrm{d}S=\\int\\frac{\\mathrm{d}q}{T}  \\hspace{2em} \\rightarrow \\hspace{2em} S = \\int\\frac{\\mathrm{d}q}{T}$\n",
    "\n",
    "$C_P=\\frac{\\mathrm{d}q}{\\mathrm{d}T}$, so $\\mathrm{d}q = C_P\\mathrm{d}T$. Now we can substitue this relation and since $C_P$ is a function of only $T$, we can integrate over temperatures to finally give:\n",
    "\n",
    "$S(298K)=\\int_{0K}^{298K}\\frac{C_P}{T}\\mathrm{d}T$\n",
    "\n",
    "And this equation is a mathematical representation of how the standard enthalpies in our general chemistry textbooks are actually measured. A sample is cooled to near $0K$, and the heat capacity is carefully measured across all temperatures. With the $C_P$ values, the integral can be calculated to give the standard molar entropy at room temperature. Note that this is exactly the process one would use to find the entropy from a statistical perspective, as described above. Although they seem like entirely different ideas ($S=k_B\\ln\\Omega$ vs $\\mathrm{d}S=\\frac{\\mathrm{d}q}{T}$), they are actually mathematically completely identical!\n",
    "\n",
    "Now, let's put this into practice. For graphitic carbon, $S^\\circ(298~K) = 5.740\\frac{J}{mol~K}$. If we put this into the statistical definition of Entropy, we can quite trivially calculate $\\Omega$, or the number of accessible configurations for one mole of graphite.\n",
    "\n",
    "$5.740\\frac{J}{K} = 1.381\\times10^{-23}\\frac{J}{K}\\ln\\Omega \\hspace{2em} \\rightarrow \\hspace{2em} \\Omega = e^{\\frac{5.740\\frac{J}{K}}{1.381\\times10^{-23}\\frac{J}{K}}}$\n",
    "\n",
    "My calculator is giving me an error, so maybe my computer can calculate this number. . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cordless-compatibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-2491e82aa678>:4: RuntimeWarning: overflow encountered in exp\n",
      "  Ω = np.exp(5.74/1.381e-23)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "S = 5.74\n",
    "k = 1.381e-23\n",
    "Ω = np.exp(5.74/1.381e-23)\n",
    "print(Ω)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-acoustic",
   "metadata": {},
   "source": [
    "Oh look, an overflow error and an output of infinity. This must be a very large number. Let's pull out the big guns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "systematic-airport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.90875779369201e+180510523253053266077388\n"
     ]
    }
   ],
   "source": [
    "from sympy import exp\n",
    "Ω = exp(5.74/1.381e-23).evalf()\n",
    "print(Ω)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-illustration",
   "metadata": {},
   "source": [
    "Oh, so $\\Omega=8.91\\times10^{180510523253053266077388}$. . . No big deal. Don't bother trying to intuitively understand the magnitude of this number. Whatever you are thinking, it is even bigger than that. Roughly, we can interpret this to mean that there are $8.91\\times10^{180510523253053266077388}$ different ways that the thermal energy can be distributed through the many different energetic modes of the graphite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-granny",
   "metadata": {},
   "source": [
    "Schrodinger, Entropy, and Biological Systems \n",
    "--------------------------------------------\n",
    "\n",
    "(or, It's *Really* Actually All About Spontanaety)\n",
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-senegal",
   "metadata": {},
   "source": [
    "Erwin Schrodinger, famous for his eponymous equation, wrote an interesting little book called \"What is Life?\" In this book he tackles many questions in biology from his physics perspective (with all due humility considering he was straying away from his field of expertise). It's fascinating to hear about the attempts to experimentally pin down the carrier molecule for genetic information (which he thought was probably a protein). For example, scientists of the time estimated the average volumetric size of a gene by exposing bacteria to radiation of different doses and noting the x-ray absorption (converted into an optical cross-section), and the rate of mutations. Their value was not too unreasonable, all before DNA had even been discovered!\n",
    "\n",
    "One of the more interesting points made by Schrodinger in his book relates to entropy in biological systems. He wrote:\n",
    "\n",
    "\n",
    ">For a while in the past our curiosity was silenced by being told that\n",
    "we feed upon energy. In some very advanced country (I don’t remember whether\n",
    "it was Germany or the U.S.A. or both) you could find menu cards in restaurants in-\n",
    "dicating, in addition to the price, the energy content of every dish. Needless to say,\n",
    "taken literally, this is just as absurd. For an adult organism the energy content is as\n",
    "stationary as the material content. Since, surely, any calorie is worth as much as any\n",
    "other calorie, one cannot see how a mere exchange could help.\n",
    ">\n",
    ">What then is that precious something contained in our food which keeps us from\n",
    "death? That is easily answered. Every process, event, happening – call it what you\n",
    "will; in a word, everything that is going on in Nature means an increase of the en-\n",
    "tropy of the part of the world where it is going on. Thus a living organism continually\n",
    "increases its entropy – or, as you may say, produces positive entropy – and thus tends\n",
    "to approach the dangerous state of maximum entropy, which is death. It can only\n",
    "keep aloof from it, i.e. alive, by continually drawing from its environment negative\n",
    "entropy – which is something very positive as we shall immediately see. What an\n",
    "organism feeds upon is negative entropy. Or, to put it less paradoxically, the essen-\n",
    "tial thing in metabolism is that the organism succeeds in freeing itself from all the\n",
    "entropy it cannot help producing while alive.\n",
    "\n",
    "What he is saying here, is that our bodies do not actually feed on energy, at least not per se. If only energy mattered, then I could take in my daily 2,500 calories in many different ways. I could stand under a hot lamp for a while (heat energy), be lifted to the top of a tall mountain (gravitational potential energy), or experience constant acceleration in an astronaut training centrifuge (kinetic energy). Of course, none of those activities will sustain my life nutritionally. It's not just that the energy needs to be released slowly. I could do those three activities gently enough to not harm my body, but they still would not help me at all. The energy needs to be in a specific *form*. It needs to be energy that my body can couple to the biochemical reactions of life in order to drive them into spontanaety. My food must contain chemicals with relatively low entropy, which can be easily converted to higher entropy products through metabolism. In short, my body needs food that contains negative entropy.\n",
    "\n",
    "The exothermic release of energy in the process of metabolising food is not always useful. We often work hard to cool our bodies down (sweating, seeking shade, etc.), but metabolism must continue even in the heat because releasing energy is not the real goal. Driving otherwise non-spontaneous reactions into spontanaety is the most important outcome of our metabolism. This driving-of-reactions may seem abstract or obscure, but we couple reactions in the lab every day. For example, copper ions reducing to copper metal is a non-spontaneous process. We can *drive* this non-spontaneous process into spontanaety by coupling it with some other reaction. Here are just two simple examples:\n",
    "* We could use a battery whose electrochemical potential is great enough to electroplate copper ions from solution. The reaction occurring inside the battery is coupled to and drives the reduction of copper in our solution. \n",
    "* We could reduce copper oxide by exposing it to methanol vapors. The oxidation of the hydrogen in methanol drives the reduction of the copper ions to produce metallic copper. \n",
    "\n",
    "Our bodies couple the reactions which metabolize our food to the formation of ATP. The dissociation of ATP can then be coupled to any other reaction our body might use to prolong our life. These reactions are not normally spontaneous. Hence. . . death can happen, and to stave off death our body must both constantly intake food (negative entropy) and couple it to those reactions to drive them (digestion/metabolism). A somewhat glib definition of death is \"a complete failure of homeostasis,\" and a close examination of any death will reveal that something has prevented the orderly driving of regular biochemical processes.\n",
    "\n",
    "It really isn't about enthalpy or energy at all, but about spontanaety. In fact, it might be possible for an organism to live on food that is *endothermic* to metabolize (food containing negative calories) as long as digesting the food is endentropic too! Such an organism could use its food to drive biochemical processes into spontanaety while taking in heat rather than releasing it. The body would cool down with greater activity rather than heat up, and the organism would need to seek sun rather than shade to maintain a homeostatic temperature. This animal would get its *calories* from raw heat, bathing in the sun or sitting too-close to a campfire, and would get its negative entropy from food. Of course such an organism doesn't exist in nature, but from a thermodynamic perspective it is certainly possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-completion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
